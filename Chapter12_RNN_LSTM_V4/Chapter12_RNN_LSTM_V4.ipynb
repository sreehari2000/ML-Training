{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from numpy import array, argmax, random, take\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import RNN, SimpleRNN, LSTM,  Embedding, RepeatVector\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "#For plotting the matplotlib graphs in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_path=\"C:\\\\Users\\\\SREEHARI\\\\Desktop\\\\internship\\\\my training\\\\Chapter12_RNN_LSTM_V4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data (5351, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column_names = ['word1', 'word2', 'word3']\n",
    "\n",
    "input_3gram = pd.read_csv(Data_path+ \"\\\\Datasets\\\\3Gram_love_data.txt\", delimiter='\\t', names=column_names) #Importing csv file with column names\n",
    "print(\"shape of data\", input_3gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few sample records from data \n",
      "      word1 word2 word3\n",
      "2360  love   him    so\n",
      "1117  love    to   see\n",
      "1257  love   him  with\n",
      "160   hate    it  when\n",
      "1791  love  them    so\n",
      "199   hate  when  that\n",
      "385   hate    to   say\n",
      "1473  love  that     i\n",
      "2553  love   the   way\n",
      "2527  love   the   way\n"
     ]
    }
   ],
   "source": [
    "print(\"Few sample records from data \\n\", input_3gram.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequency of word1 values \n",
      " love      4327\n",
      "loved      416\n",
      "hate       400\n",
      "hated       80\n",
      "loves       72\n",
      "lovely      24\n",
      "loving      24\n",
      "hates        8\n",
      "Name: word1, dtype: int64\n",
      "\n",
      "Frequency of word2 values \n",
      " to         1866\n",
      "it         1361\n",
      "the         548\n",
      "with        240\n",
      "him         144\n",
      "you         144\n",
      "of          136\n",
      "her         104\n",
      "for          96\n",
      "and          88\n",
      "what         56\n",
      "is           48\n",
      "each         40\n",
      "in           40\n",
      "ones         32\n",
      "me           32\n",
      "nothing      32\n",
      "them         32\n",
      "as           24\n",
      "every        24\n",
      "more         16\n",
      "going        16\n",
      "that         16\n",
      "being        16\n",
      "affair       16\n",
      "my           16\n",
      "about         8\n",
      "your          8\n",
      "on            8\n",
      "letter        8\n",
      "most          8\n",
      "thy           8\n",
      "view          8\n",
      "song          8\n",
      "makes         8\n",
      "got           8\n",
      "this          8\n",
      "at            8\n",
      "a             8\n",
      "one           8\n",
      "hearing       8\n",
      "story         8\n",
      "all           8\n",
      "lost          8\n",
      "man           8\n",
      "when          8\n",
      "husband       8\n",
      "Name: word2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFrequency of word1 values \\n\", input_3gram[\"word1\"].value_counts())\n",
    "print(\"\\nFrequency of word2 values \\n\", input_3gram[\"word2\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique words overall: 139\n",
      "unique words list: ['a' 'able' 'about' 'admit' 'affair' 'affection' 'all' 'and' 'another'\n",
      " 'answer' 'as' 'at' 'be' 'because' 'being' 'better' 'between' 'bother'\n",
      " 'break' 'care' 'cared' 'come' 'concern' 'country' 'cut' 'disappoint' 'do'\n",
      " 'each' 'every' 'fact' 'feel' 'feeling' 'find' 'first' 'for' 'from' 'get'\n",
      " 'go' 'god' 'going' 'got' 'hate' 'hated' 'hates' 'have' 'he' 'hear'\n",
      " 'hearing' 'her' 'here' 'him' 'his' 'husband' 'i' 'idea' 'if' 'in'\n",
      " 'interrupt' 'is' 'it' 'kind' 'know' 'leave' 'letter' 'life' 'like'\n",
      " 'listen' 'look' 'lost' 'lot' 'love' 'loved' 'lovely' 'loves' 'loving'\n",
      " 'make' 'makes' 'man' 'marriage' 'me' 'minute' 'more' 'most' 'much'\n",
      " 'music' 'my' 'nature' 'neighbor' 'not' 'nothing' 'of' 'on' 'one' 'ones'\n",
      " 'or' 'other' 'over' 'play' 'respect' 'say' 'see' 'sit' 'smell' 'so'\n",
      " 'someone' 'song' 'sound' 'story' 'stronger' 'support' 'take' 'talk'\n",
      " 'tell' 'than' 'that' 'the' 'them' 'they' 'think' 'this' 'thought' 'thy'\n",
      " 'to' 'too' 'united' 'use' 'very' 'view' 'watch' 'way' 'we' 'what' 'when'\n",
      " 'wife' 'will' 'with' 'work' 'you' 'your']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finding our words to create dictionary\n",
    "Here we find unique values in each column and save each of those values .\n",
    "Later which we will take the unique value for the entire appened columns\n",
    "This will be our vocabulary list,which are the unique words in our data file\n",
    "\"\"\"\n",
    "unique_words = []\n",
    "for i in list(input_3gram.columns.values):\n",
    "    for j in pd.unique(input_3gram[i]):\n",
    "        unique_words.append(j)\n",
    "unique_words = np.unique(unique_words)\n",
    "\n",
    "\n",
    "print('Count of unique words overall:', len(unique_words))\n",
    "print('unique words list:', unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_indices dictionary \n",
      " {'a': 0, 'able': 1, 'about': 2, 'admit': 3, 'affair': 4, 'affection': 5, 'all': 6, 'and': 7, 'another': 8, 'answer': 9, 'as': 10, 'at': 11, 'be': 12, 'because': 13, 'being': 14, 'better': 15, 'between': 16, 'bother': 17, 'break': 18, 'care': 19, 'cared': 20, 'come': 21, 'concern': 22, 'country': 23, 'cut': 24, 'disappoint': 25, 'do': 26, 'each': 27, 'every': 28, 'fact': 29, 'feel': 30, 'feeling': 31, 'find': 32, 'first': 33, 'for': 34, 'from': 35, 'get': 36, 'go': 37, 'god': 38, 'going': 39, 'got': 40, 'hate': 41, 'hated': 42, 'hates': 43, 'have': 44, 'he': 45, 'hear': 46, 'hearing': 47, 'her': 48, 'here': 49, 'him': 50, 'his': 51, 'husband': 52, 'i': 53, 'idea': 54, 'if': 55, 'in': 56, 'interrupt': 57, 'is': 58, 'it': 59, 'kind': 60, 'know': 61, 'leave': 62, 'letter': 63, 'life': 64, 'like': 65, 'listen': 66, 'look': 67, 'lost': 68, 'lot': 69, 'love': 70, 'loved': 71, 'lovely': 72, 'loves': 73, 'loving': 74, 'make': 75, 'makes': 76, 'man': 77, 'marriage': 78, 'me': 79, 'minute': 80, 'more': 81, 'most': 82, 'much': 83, 'music': 84, 'my': 85, 'nature': 86, 'neighbor': 87, 'not': 88, 'nothing': 89, 'of': 90, 'on': 91, 'one': 92, 'ones': 93, 'or': 94, 'other': 95, 'over': 96, 'play': 97, 'respect': 98, 'say': 99, 'see': 100, 'sit': 101, 'smell': 102, 'so': 103, 'someone': 104, 'song': 105, 'sound': 106, 'story': 107, 'stronger': 108, 'support': 109, 'take': 110, 'talk': 111, 'tell': 112, 'than': 113, 'that': 114, 'the': 115, 'them': 116, 'they': 117, 'think': 118, 'this': 119, 'thought': 120, 'thy': 121, 'to': 122, 'too': 123, 'united': 124, 'use': 125, 'very': 126, 'view': 127, 'watch': 128, 'way': 129, 'we': 130, 'what': 131, 'when': 132, 'wife': 133, 'will': 134, 'with': 135, 'work': 136, 'you': 137, 'your': 138}\n",
      "word_indices.keys \n",
      " dict_keys(['a', 'able', 'about', 'admit', 'affair', 'affection', 'all', 'and', 'another', 'answer', 'as', 'at', 'be', 'because', 'being', 'better', 'between', 'bother', 'break', 'care', 'cared', 'come', 'concern', 'country', 'cut', 'disappoint', 'do', 'each', 'every', 'fact', 'feel', 'feeling', 'find', 'first', 'for', 'from', 'get', 'go', 'god', 'going', 'got', 'hate', 'hated', 'hates', 'have', 'he', 'hear', 'hearing', 'her', 'here', 'him', 'his', 'husband', 'i', 'idea', 'if', 'in', 'interrupt', 'is', 'it', 'kind', 'know', 'leave', 'letter', 'life', 'like', 'listen', 'look', 'lost', 'lot', 'love', 'loved', 'lovely', 'loves', 'loving', 'make', 'makes', 'man', 'marriage', 'me', 'minute', 'more', 'most', 'much', 'music', 'my', 'nature', 'neighbor', 'not', 'nothing', 'of', 'on', 'one', 'ones', 'or', 'other', 'over', 'play', 'respect', 'say', 'see', 'sit', 'smell', 'so', 'someone', 'song', 'sound', 'story', 'stronger', 'support', 'take', 'talk', 'tell', 'than', 'that', 'the', 'them', 'they', 'think', 'this', 'thought', 'thy', 'to', 'too', 'united', 'use', 'very', 'view', 'watch', 'way', 'we', 'what', 'when', 'wife', 'will', 'with', 'work', 'you', 'your'])\n",
      "word_indices.values \n",
      " dict_values([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138])\n",
      "\n",
      " ########################################\n",
      "\n",
      "indices_words dictionary \n",
      " {0: 'a', 1: 'able', 2: 'about', 3: 'admit', 4: 'affair', 5: 'affection', 6: 'all', 7: 'and', 8: 'another', 9: 'answer', 10: 'as', 11: 'at', 12: 'be', 13: 'because', 14: 'being', 15: 'better', 16: 'between', 17: 'bother', 18: 'break', 19: 'care', 20: 'cared', 21: 'come', 22: 'concern', 23: 'country', 24: 'cut', 25: 'disappoint', 26: 'do', 27: 'each', 28: 'every', 29: 'fact', 30: 'feel', 31: 'feeling', 32: 'find', 33: 'first', 34: 'for', 35: 'from', 36: 'get', 37: 'go', 38: 'god', 39: 'going', 40: 'got', 41: 'hate', 42: 'hated', 43: 'hates', 44: 'have', 45: 'he', 46: 'hear', 47: 'hearing', 48: 'her', 49: 'here', 50: 'him', 51: 'his', 52: 'husband', 53: 'i', 54: 'idea', 55: 'if', 56: 'in', 57: 'interrupt', 58: 'is', 59: 'it', 60: 'kind', 61: 'know', 62: 'leave', 63: 'letter', 64: 'life', 65: 'like', 66: 'listen', 67: 'look', 68: 'lost', 69: 'lot', 70: 'love', 71: 'loved', 72: 'lovely', 73: 'loves', 74: 'loving', 75: 'make', 76: 'makes', 77: 'man', 78: 'marriage', 79: 'me', 80: 'minute', 81: 'more', 82: 'most', 83: 'much', 84: 'music', 85: 'my', 86: 'nature', 87: 'neighbor', 88: 'not', 89: 'nothing', 90: 'of', 91: 'on', 92: 'one', 93: 'ones', 94: 'or', 95: 'other', 96: 'over', 97: 'play', 98: 'respect', 99: 'say', 100: 'see', 101: 'sit', 102: 'smell', 103: 'so', 104: 'someone', 105: 'song', 106: 'sound', 107: 'story', 108: 'stronger', 109: 'support', 110: 'take', 111: 'talk', 112: 'tell', 113: 'than', 114: 'that', 115: 'the', 116: 'them', 117: 'they', 118: 'think', 119: 'this', 120: 'thought', 121: 'thy', 122: 'to', 123: 'too', 124: 'united', 125: 'use', 126: 'very', 127: 'view', 128: 'watch', 129: 'way', 130: 'we', 131: 'what', 132: 'when', 133: 'wife', 134: 'will', 135: 'with', 136: 'work', 137: 'you', 138: 'your'}\n",
      "indices_words keys \n",
      " dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138])\n",
      "indices_words values \n",
      " dict_values(['a', 'able', 'about', 'admit', 'affair', 'affection', 'all', 'and', 'another', 'answer', 'as', 'at', 'be', 'because', 'being', 'better', 'between', 'bother', 'break', 'care', 'cared', 'come', 'concern', 'country', 'cut', 'disappoint', 'do', 'each', 'every', 'fact', 'feel', 'feeling', 'find', 'first', 'for', 'from', 'get', 'go', 'god', 'going', 'got', 'hate', 'hated', 'hates', 'have', 'he', 'hear', 'hearing', 'her', 'here', 'him', 'his', 'husband', 'i', 'idea', 'if', 'in', 'interrupt', 'is', 'it', 'kind', 'know', 'leave', 'letter', 'life', 'like', 'listen', 'look', 'lost', 'lot', 'love', 'loved', 'lovely', 'loves', 'loving', 'make', 'makes', 'man', 'marriage', 'me', 'minute', 'more', 'most', 'much', 'music', 'my', 'nature', 'neighbor', 'not', 'nothing', 'of', 'on', 'one', 'ones', 'or', 'other', 'over', 'play', 'respect', 'say', 'see', 'sit', 'smell', 'so', 'someone', 'song', 'sound', 'story', 'stronger', 'support', 'take', 'talk', 'tell', 'than', 'that', 'the', 'them', 'they', 'think', 'this', 'thought', 'thy', 'to', 'too', 'united', 'use', 'very', 'view', 'watch', 'way', 'we', 'what', 'when', 'wife', 'will', 'with', 'work', 'you', 'your'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "creating our word:indice pair dictionary and inverse\n",
    "Here will be creating two dictonary values\n",
    "word_indices : This contains each words mapped to an unique digit \n",
    "indices_words : This contains each digits mapped to a word in the same sequence as word_indices \n",
    "\"\"\"\n",
    "word_indices = dict((w, i) for i, w in enumerate(unique_words))\n",
    "indices_words = dict((i, w) for i, w in enumerate(unique_words))\n",
    "\n",
    "print(\"word_indices dictionary \\n\",word_indices)\n",
    "print(\"word_indices.keys \\n\", word_indices.keys())\n",
    "print(\"word_indices.values \\n\", word_indices.values())\n",
    "print(\"\\n ########################################\\n\")\n",
    "print(\"indices_words dictionary \\n\", indices_words)\n",
    "print(\"indices_words keys \\n\",indices_words.keys())\n",
    "print(\"indices_words values \\n\",indices_words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1_onehot shape is  (5351, 139)\n"
     ]
    }
   ],
   "source": [
    "### Onehot encoding of word1\n",
    "word1 = input_3gram['word1'].map(word_indices)\n",
    "word1_onehot = keras.utils.to_categorical(np.array(word1), num_classes=len(word_indices))\n",
    "print(\"word1_onehot shape is \",word1_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word in row 0 is -->hate\n",
      "The one hot encoded version of the word in row 0 is \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "The word in row 500 is --> love\n",
      "The one hot encoded version of the word in row 500 is \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Lets take example of two different words\n",
    "print(\"The word in row 0 is -->\"+input_3gram['word1'][0])\n",
    "print(\"The one hot encoded version of the word in row 0 is \\n\",word1_onehot[0])\n",
    "\n",
    "print(\"\\nThe word in row 500 is --> \"+input_3gram['word1'][500])\n",
    "print(\"The one hot encoded version of the word in row 500 is \\n\",word1_onehot[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2_onehot shape is  (5351, 139)\n",
      "word3_onehot shape is  (5351, 139)\n"
     ]
    }
   ],
   "source": [
    "##one hot encoding for word2 and word3 \n",
    "word2 = input_3gram['word2'].map(word_indices)\n",
    "word2_onehot = keras.utils.to_categorical(np.array(word2), num_classes=len(word_indices))\n",
    "print(\"word2_onehot shape is \",word2_onehot.shape)\n",
    "\n",
    "word3 = input_3gram['word3'].map(word_indices)\n",
    "word3_onehot = keras.utils.to_categorical(np.array(word3), num_classes=len(word_indices))\n",
    "print(\"word3_onehot shape is \",word3_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                1400      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 139)               1529      \n",
      "=================================================================\n",
      "Total params: 2,929\n",
      "Trainable params: 2,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ANN_model1 = Sequential()\n",
    "ANN_model1.add(Dense(10, input_dim=word1_onehot.shape[1], activation='sigmoid'))\n",
    "ANN_model1.add(Dense(word2_onehot.shape[1] ,activation='softmax'))\n",
    "ANN_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 2ms/step - loss: 0.5536 - accuracy: 0.0015\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3207 - accuracy: 0.0639\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1870 - accuracy: 0.3487\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1194 - accuracy: 0.3487\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0844 - accuracy: 0.3487\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0649 - accuracy: 0.3487\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.3487\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 0.3487\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.3487\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.3487\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 0.3487\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0321 - accuracy: 0.3487\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 0.3487\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 0.3487\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 0.3487\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.3487\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.3487\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.3487\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.3487\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.3487\n"
     ]
    }
   ],
   "source": [
    "ANN_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = ANN_model1.fit(word1_onehot, word2_onehot, epochs=20, batch_size=50,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will see what the 1st hidden layer output representation of the data  \n",
    "# to predict the hidden layer activations, \n",
    "# let's rewrite first layer of our model and give it the weights from fully trained previous model\n",
    "model1_hidden = Sequential()\n",
    "model1_hidden.add(Dense(10, input_dim=word1_onehot.shape[1], weights=ANN_model1.layers[0].get_weights()))\n",
    "model1_hidden.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hidden layer output for every record - Shape of it \n",
      " (5351, 10)\n",
      "Few five records from hidden layer \n",
      " [[0.8334368  0.84318197 0.8652694  0.8443494  0.8454185  0.8570777\n",
      "  0.84676427 0.8684742  0.8556158  0.8603386 ]\n",
      " [0.8334368  0.84318197 0.8652694  0.8443494  0.8454185  0.8570777\n",
      "  0.84676427 0.8684742  0.8556158  0.8603386 ]\n",
      " [0.8334368  0.84318197 0.8652694  0.8443494  0.8454185  0.8570777\n",
      "  0.84676427 0.8684742  0.8556158  0.8603386 ]\n",
      " [0.8334368  0.84318197 0.8652694  0.8443494  0.8454185  0.8570777\n",
      "  0.84676427 0.8684742  0.8556158  0.8603386 ]\n",
      " [0.8334368  0.84318197 0.8652694  0.8443494  0.8454185  0.8570777\n",
      "  0.84676427 0.8684742  0.8556158  0.8603386 ]]\n"
     ]
    }
   ],
   "source": [
    "# Getting the hidden layer activations\n",
    "model1_hidden_output = model1_hidden.predict(word1_onehot)\n",
    "#peak into our hidden layer activations\n",
    "print(\"The hidden layer output for every record - Shape of it \\n\", model1_hidden_output.shape)\n",
    "print(\"Few five records from hidden layer \\n\",model1_hidden_output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2_hidden_append Shape (5351, 149)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We append the input words of the words2 column in the output of the h1 layer,this gives us the combined input representation\n",
    "\"\"\"\n",
    "word2_hidden_append = np.append(model1_hidden_output,word2_onehot, axis=1)\n",
    "print(\"word2_hidden_append Shape\", word2_hidden_append.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10)                1500      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 139)               1529      \n",
      "=================================================================\n",
      "Total params: 3,029\n",
      "Trainable params: 3,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ANN_model2 = Sequential()\n",
    "ANN_model2.add(Dense(10, input_dim=word2_hidden_append.shape[1], activation='sigmoid'))\n",
    "ANN_model2.add(Dense(word3_onehot.shape[1], activation='softmax'))\n",
    "ANN_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5060 - accuracy: 0.0015\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.2260 - accuracy: 0.0890\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.2439\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0811 - accuracy: 0.2439\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0627 - accuracy: 0.2439\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.2439\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0464 - accuracy: 0.2439\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0424 - accuracy: 0.2439\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0396 - accuracy: 0.2439\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.2439\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.2439\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.2439\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0342 - accuracy: 0.2439\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0335 - accuracy: 0.2439\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 0.2439\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.2439\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.2439\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0319 - accuracy: 0.2439\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0317 - accuracy: 0.2439\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0315 - accuracy: 0.2439\n"
     ]
    }
   ],
   "source": [
    "ANN_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = ANN_model2.fit(word2_hidden_append, word3_onehot, epochs=20, batch_size=50,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A predict function that takes input word1 and word2; and predict word3 \n",
    "#1. take the input word , and represent them using digits from the word_indices dictonary values\n",
    "#2. getting the intermediate hidden nodes for word1\n",
    "#3. appending hidden activations with word2 as final test set\n",
    "#4. prediction on this test set\n",
    "def two_step_pred(words_in):\n",
    "\n",
    "    index_input=word_indices[words_in[0]]\n",
    "    indices_in = keras.utils.to_categorical(index_input, num_classes=len(word_indices))\n",
    "    indices_in=indices_in.reshape(1,len(word_indices))\n",
    "    h1_test = model1_hidden.predict(indices_in) # getting our intermediate hidden activations from model1h\n",
    "    \n",
    "    \n",
    "    index_input2=word_indices[words_in[1]]\n",
    "    indices_in2 = keras.utils.to_categorical(index_input2, num_classes=len(word_indices))\n",
    "    indices_in2= indices_in2.reshape(1,len(word_indices))\n",
    "    X2_test = np.append(h1_test, indices_in2, axis=1) #preparing final test data by appending hidden with word2\n",
    "    \n",
    "    yhat = ANN_model2.predict_classes(X2_test) #predicting final output from model2\n",
    "    \n",
    "    print(\"Input words --> \", words_in)\n",
    "    print(\"Predicted word --> \", indices_words[yhat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input words -->  ['love', 'it']\n",
      "Predicted word -->  when\n",
      "Input words -->  ['love', 'to']\n",
      "Predicted word -->  when\n",
      "Input words -->  ['love', 'the']\n",
      "Predicted word -->  when\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEHARI\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "two_step_pred(['love', 'it'])\n",
    "two_step_pred(['love', 'to'])\n",
    "two_step_pred(['love', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 4)                 24        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 12        \n",
      "=================================================================\n",
      "Total params: 36\n",
      "Trainable params: 36\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, use_bias=False, input_shape=(2,2)))\n",
    "model.add(Dense(3, use_bias=False, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 43\n",
      "Trainable params: 43\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, input_shape=(2,2)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 43\n",
      "Trainable params: 43\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, input_shape=(4,2)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word prediction using RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1_word2_onehot shape (5351, 2, 139)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEHARI\\AppData\\Local\\Temp\\ipykernel_24376\\3906608908.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word1_word2[i] = word1_word2[i].map(word_indices)\n",
      "C:\\Users\\SREEHARI\\AppData\\Local\\Temp\\ipykernel_24376\\3906608908.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  word1_word2[i] = word1_word2[i].map(word_indices)\n"
     ]
    }
   ],
   "source": [
    "word1_word2 = input_3gram[['word1','word2']]\n",
    "for i in list(word1_word2.columns.values):\n",
    "    word1_word2[i] = word1_word2[i].map(word_indices)\n",
    "\n",
    "word1_word2=np.array(word1_word2)\n",
    "#The same data is reshaped with similar structure but appended with 1 value to make it 3d array\n",
    "word1_word2=np.reshape(word1_word2,(word1_word2.shape[0],2,1))\n",
    "word1_word2_onehot = keras.utils.to_categorical(np.array(word1_word2), num_classes=len(word_indices))\n",
    "print(\"word1_word2_onehot shape\", word1_word2_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps 2\n",
      "Input nodes 139\n",
      "output nodes 139\n"
     ]
    }
   ],
   "source": [
    "print(\"time steps\" , word1_word2_onehot.shape[1])\n",
    "print(\"Input nodes\" , word1_word2_onehot.shape[2])\n",
    "print(\"output nodes\" , word3_onehot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (None, 30)                5100      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 139)               4309      \n",
      "=================================================================\n",
      "Total params: 9,409\n",
      "Trainable params: 9,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn = Sequential()\n",
    "#model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, input_data_dim)))\n",
    "model_rnn.add(SimpleRNN(30, input_shape=(word1_word2_onehot.shape[1],word1_word2_onehot.shape[2]))) \n",
    "model_rnn.add(Dense(word3_onehot.shape[1], activation='softmax'))\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 3.7604 - accuracy: 0.2801\n",
      "Epoch 2/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 2.8047 - accuracy: 0.4743\n",
      "Epoch 3/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 2.3615 - accuracy: 0.5317\n",
      "Epoch 4/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 2.0610 - accuracy: 0.5689\n",
      "Epoch 5/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.8732 - accuracy: 0.5905\n",
      "Epoch 6/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.7375 - accuracy: 0.6066\n",
      "Epoch 7/20\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.6322 - accuracy: 0.6225\n",
      "Epoch 8/20\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.5455 - accuracy: 0.6361\n",
      "Epoch 9/20\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.4739 - accuracy: 0.6481\n",
      "Epoch 10/20\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.4153 - accuracy: 0.6567\n",
      "Epoch 11/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.3658 - accuracy: 0.6597\n",
      "Epoch 12/20\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.3262 - accuracy: 0.6640\n",
      "Epoch 13/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.2956 - accuracy: 0.6627\n",
      "Epoch 14/20\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.2701 - accuracy: 0.6645\n",
      "Epoch 15/20\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.2495 - accuracy: 0.6631\n",
      "Epoch 16/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.2337 - accuracy: 0.6645\n",
      "Epoch 17/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.2203 - accuracy: 0.6655\n",
      "Epoch 18/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.2100 - accuracy: 0.6625\n",
      "Epoch 19/20\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 1.2009 - accuracy: 0.6651\n",
      "Epoch 20/20\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 1.1931 - accuracy: 0.6634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e2282b2520>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_rnn.fit(word1_word2_onehot, word3_onehot, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_word_pred(in_text):\n",
    "    print(\"Input is - \" , in_text)\n",
    "    encoded = [word_indices[i] for i in in_text]\n",
    "    encoded = np.array(encoded).reshape(1,2,1)\n",
    "    encoded =keras.utils.to_categorical(np.array(encoded), num_classes=len(word_indices))\n",
    "    ypred = model_rnn.predict_classes(encoded, verbose=0)[0]\n",
    "    print(\"Output is --> \" ,indices_words[ypred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is -  ['love', 'it']\n",
      "Output is -->  when\n",
      "Input is -  ['love', 'to']\n",
      "Output is -->  see\n",
      "Input is -  ['love', 'the']\n",
      "Output is -->  way\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEHARI\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "rnn_word_pred(['love', 'it'])\n",
    "rnn_word_pred(['love', 'to'])\n",
    "rnn_word_pred(['love', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Long Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,addresses,of\n",
      "and,adherence,to\n",
      "and,advocates,for\n",
      "and,aerospace,engineering\n",
      "and,americans,do\n",
      "and,analyzing,the\n",
      "and,announced,he\n",
      "and,announced,he\n",
      "and,announced,plans\n",
      "and,announced,that\n",
      "and,announced,that\n",
      "and,annou\n"
     ]
    }
   ],
   "source": [
    "longseq_3gram = open(Data_path+'\\\\Long_sequence_3gram.csv').read().lower()\n",
    "print(longseq_3gram[495:801])\n",
    "print(longseq_3gram[30615:31000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and addresses \n"
     ]
    }
   ],
   "source": [
    "#Replace comma with space\n",
    "longseq_3gram1= longseq_3gram.replace(',',' ').replace('\\r','')\n",
    "print(longseq_3gram1[495:750])\n",
    "print(longseq_3gram1[30615:30800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Characters in the text \n",
      "  ['\\n', ' ', \"'\", '(', '-', '.', '/', '0', '1', '3', '7', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      " Character after removing newline symbol '\\n' [' ', \"'\", '(', '-', '.', '/', '0', '1', '3', '7', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      " overall chars count 37\n"
     ]
    }
   ],
   "source": [
    "#Unique characters in our dataset we then sort it\n",
    "chars = sorted(list(set(longseq_3gram1)))\n",
    "print(\"Unique Characters in the text \\n \",chars)\n",
    "#\\n is character string for new line, we dont need that in our dictionary of chars\n",
    "chars.remove('\\n')\n",
    "print(\"\\n Character after removing newline symbol \\'\\\\n\\'\",chars)\n",
    "print(\"\\n overall chars count\", len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters to indices dictionary\n",
      " {' ': 0, \"'\": 1, '(': 2, '-': 3, '.': 4, '/': 5, '0': 6, '1': 7, '3': 8, '7': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36}\n",
      "indices to char dictionary\n",
      " {0: ' ', 1: \"'\", 2: '(', 3: '-', 4: '.', 5: '/', 6: '0', 7: '1', 8: '3', 9: '7', 10: '9', 11: 'a', 12: 'b', 13: 'c', 14: 'd', 15: 'e', 16: 'f', 17: 'g', 18: 'h', 19: 'i', 20: 'j', 21: 'k', 22: 'l', 23: 'm', 24: 'n', 25: 'o', 26: 'p', 27: 'q', 28: 'r', 29: 's', 30: 't', 31: 'u', 32: 'v', 33: 'w', 34: 'x', 35: 'y', 36: 'z'}\n",
      "unique chars:  {37}\n"
     ]
    }
   ],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "print(\"characters to indices dictionary\\n\", char_indices)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(\"indices to char dictionary\\n\", indices_char)\n",
    "print('unique chars: ', {len(chars)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char to index on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bewildering array  [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11, 35, 0]\n",
      "a celebration of  [11, 0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25, 16, 0]\n",
      "a co-director of  [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15, 30, 35, 0]\n",
      "a declaration of  [11, 0, 12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0, 23, 25, 23, 15, 24, 30, 0]\n",
      "a significant risk  [11, 0, 29, 19, 17, 24, 19, 16, 19, 13, 11, 24, 30, 0, 28, 19, 29, 21, 0]\n",
      "been designed as  [12, 15, 15, 24, 0, 14, 15, 29, 19, 17, 24, 15, 14, 0, 11, 29, 0]\n",
      "from anywhere on  [16, 28, 25, 23, 0, 11, 24, 35, 33, 18, 15, 28, 15, 0, 25, 24, 0]\n",
      "Number of sentences  30307\n"
     ]
    }
   ],
   "source": [
    "data = longseq_3gram1.splitlines()\n",
    "##Adding a space at the end\n",
    "data = [i+' ' for i in data]\n",
    "\n",
    "##mapping our data into numbers\n",
    "sentences = [[char_indices[j] for j in i] for i in data ]\n",
    "print(data[0], sentences[0])\n",
    "print(data[10], sentences[1])\n",
    "print(data[20], sentences[2])\n",
    "print(data[100], sentences[3])\n",
    "print(data[400], sentences[400])\n",
    "print(data[4000], sentences[4000])\n",
    "print(data[9000], sentences[9000])\n",
    "##Number of sentences\n",
    "print(\"Number of sentences \", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the sentences to RNN friendly data which can be used to generate new sequences\n",
    "\n",
    "**Take one sentence, iterate through it till the length of sentence is reached:**\n",
    "\n",
    "* **Step 1** 0:0+14: X; and 14th position: y >> observation 1\n",
    "* **Step 2** 1:14: X; and 15th position: y >> observation 2\n",
    "* **Step 3** we do this till the length of sentence is reached\n",
    "\n",
    "Take next sentence and repeat the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142142, 142142)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since all the sentences may not be of same length,it is neccessary to make them consistent when passing to keras\n",
    "#We select a sequence length\n",
    "Seq_ln = 14\n",
    "X = []\n",
    "y = []\n",
    "for i in sentences:\n",
    "    for j in range(len(i)-Seq_ln):\n",
    "        X.append(i[j:j+Seq_ln])\n",
    "        y.append(i[j+Seq_ln])\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0:2]= ['a bewildering array ', 'a beneficiary of ']\n",
      "sentences[0:2]= [[11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11, 35, 0], [11, 0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25, 16, 0]]\n",
      "X[ 0 ]= [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0] y[ 0 ]= 11\n",
      "X[ 1 ]= [0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11] y[ 1 ]= 28\n",
      "X[ 2 ]= [12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28] y[ 2 ]= 28\n",
      "X[ 3 ]= [15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28] y[ 3 ]= 11\n",
      "X[ 4 ]= [33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11] y[ 4 ]= 35\n",
      "X[ 5 ]= [19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11, 35] y[ 5 ]= 0\n",
      "X[ 6 ]= [11, 0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0] y[ 6 ]= 25\n",
      "X[ 7 ]= [0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25] y[ 7 ]= 16\n",
      "X[ 8 ]= [12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25, 16] y[ 8 ]= 0\n",
      "X[ 9 ]= [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0] y[ 9 ]= 32\n",
      "X[ 10 ]= [0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32] y[ 10 ]= 11\n",
      "X[ 11 ]= [12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11] y[ 11 ]= 28\n",
      "X[ 12 ]= [15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28] y[ 12 ]= 19\n",
      "X[ 13 ]= [33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19] y[ 13 ]= 15\n",
      "X[ 14 ]= [19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15] y[ 14 ]= 30\n",
      "X[ 15 ]= [22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15, 30] y[ 15 ]= 35\n",
      "X[ 16 ]= [14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15, 30, 35] y[ 16 ]= 0\n",
      "X[ 17 ]= [11, 0, 12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0] y[ 17 ]= 23\n",
      "X[ 18 ]= [0, 12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0, 23] y[ 18 ]= 25\n",
      "X[ 19 ]= [12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0, 23, 25] y[ 19 ]= 23\n"
     ]
    }
   ],
   "source": [
    "print(\"data[0:2]=\", data[0:2])\n",
    "print(\"sentences[0:2]=\", sentences[0:2])\n",
    "\n",
    "for i in range (0,20):\n",
    "    print(\"X[\",i,\"]=\", X[i],\"y[\",i,\"]=\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142142, 14, 37)\n"
     ]
    }
   ],
   "source": [
    "#The first row is the X's first row up to 14 character\n",
    "#The second row is the X's first row starting from second character up to 14 character\n",
    "#The third row is the X's first row starting from third character up to 14 character and so on \n",
    "X=np.array(X)\n",
    "X1=np.reshape(X,(X.shape[0],X.shape[1],1))\n",
    "X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142142, 37)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Target Variable\n",
    "y[:10]\n",
    "#Reshapig our label for model\n",
    "y1 = np.array(y)\n",
    "# one hot encode outputs\n",
    "y1 = keras.utils.to_categorical(np.array(y), num_classes=len(char_indices))\n",
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (113713, 14, 37)\n",
      "y_train.shape (113713, 37)\n",
      "X_test.shape (28429, 14, 37)\n",
      "y_test.shape (28429, 37)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.20)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_4 (SimpleRNN)     (None, 16)                864       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 37)                629       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 37)                0         \n",
      "=================================================================\n",
      "Total params: 1,493\n",
      "Trainable params: 1,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building the model\n",
    "model_RNN2 = Sequential()\n",
    "##model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, data_dim)))\n",
    "model_RNN2.add(SimpleRNN(16, input_shape=(X_train.shape[1], X_train.shape[2]))) \n",
    "model_RNN2.add(Dense(len(char_indices)))\n",
    "model_RNN2.add(Activation('softmax'))\n",
    "model_RNN2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3554/3554 [==============================] - 38s 10ms/step - loss: 2.2602 - accuracy: 0.3648 - val_loss: 1.9787 - val_accuracy: 0.4235\n",
      "Epoch 2/30\n",
      "3554/3554 [==============================] - 37s 10ms/step - loss: 1.9122 - accuracy: 0.4432 - val_loss: 1.8757 - val_accuracy: 0.4456\n",
      "Epoch 3/30\n",
      "3554/3554 [==============================] - 37s 10ms/step - loss: 1.8392 - accuracy: 0.4596 - val_loss: 1.8249 - val_accuracy: 0.4625\n",
      "Epoch 4/30\n",
      "3554/3554 [==============================] - 37s 10ms/step - loss: 1.7992 - accuracy: 0.4714 - val_loss: 1.7914 - val_accuracy: 0.4702\n",
      "Epoch 5/30\n",
      "3554/3554 [==============================] - 37s 10ms/step - loss: 1.7723 - accuracy: 0.4792 - val_loss: 1.7694 - val_accuracy: 0.4788\n",
      "Epoch 6/30\n",
      "3554/3554 [==============================] - 38s 11ms/step - loss: 1.7526 - accuracy: 0.4836 - val_loss: 1.7520 - val_accuracy: 0.4848\n",
      "Epoch 7/30\n",
      "3554/3554 [==============================] - 36s 10ms/step - loss: 1.7380 - accuracy: 0.4868 - val_loss: 1.7394 - val_accuracy: 0.4860\n",
      "Epoch 8/30\n",
      "3554/3554 [==============================] - 37s 11ms/step - loss: 1.7252 - accuracy: 0.4890 - val_loss: 1.7280 - val_accuracy: 0.4862\n",
      "Epoch 9/30\n",
      "3554/3554 [==============================] - 37s 10ms/step - loss: 1.7151 - accuracy: 0.4903 - val_loss: 1.7190 - val_accuracy: 0.4895\n",
      "Epoch 10/30\n",
      "3554/3554 [==============================] - 37s 10ms/step - loss: 1.7064 - accuracy: 0.4930 - val_loss: 1.7103 - val_accuracy: 0.4906\n",
      "Epoch 11/30\n",
      "3554/3554 [==============================] - 33s 9ms/step - loss: 1.6987 - accuracy: 0.4961 - val_loss: 1.7039 - val_accuracy: 0.4925\n",
      "Epoch 12/30\n",
      "3554/3554 [==============================] - 34s 10ms/step - loss: 1.6924 - accuracy: 0.4960 - val_loss: 1.6975 - val_accuracy: 0.4951\n",
      "Epoch 13/30\n",
      "3554/3554 [==============================] - 38s 11ms/step - loss: 1.6863 - accuracy: 0.4984 - val_loss: 1.6951 - val_accuracy: 0.4962\n",
      "Epoch 14/30\n",
      "3554/3554 [==============================] - 37s 10ms/step - loss: 1.6810 - accuracy: 0.4995 - val_loss: 1.6894 - val_accuracy: 0.4942\n",
      "Epoch 15/30\n",
      "3554/3554 [==============================] - 37s 11ms/step - loss: 1.6764 - accuracy: 0.5006 - val_loss: 1.6880 - val_accuracy: 0.4993\n",
      "Epoch 16/30\n",
      "3554/3554 [==============================] - 38s 11ms/step - loss: 1.6723 - accuracy: 0.5024 - val_loss: 1.6817 - val_accuracy: 0.4998\n",
      "Epoch 17/30\n",
      "3554/3554 [==============================] - 38s 11ms/step - loss: 1.6688 - accuracy: 0.5038 - val_loss: 1.6792 - val_accuracy: 0.4977\n",
      "Epoch 18/30\n",
      "3554/3554 [==============================] - 38s 11ms/step - loss: 1.6657 - accuracy: 0.5047 - val_loss: 1.6762 - val_accuracy: 0.4986\n",
      "Epoch 19/30\n",
      "3554/3554 [==============================] - 40s 11ms/step - loss: 1.6627 - accuracy: 0.5059 - val_loss: 1.6733 - val_accuracy: 0.5046\n",
      "Epoch 20/30\n",
      "3554/3554 [==============================] - 41s 12ms/step - loss: 1.6602 - accuracy: 0.5070 - val_loss: 1.6698 - val_accuracy: 0.5010\n",
      "Epoch 21/30\n",
      "3554/3554 [==============================] - 41s 12ms/step - loss: 1.6581 - accuracy: 0.5071 - val_loss: 1.6689 - val_accuracy: 0.5014\n",
      "Epoch 22/30\n",
      "3554/3554 [==============================] - 36s 10ms/step - loss: 1.6559 - accuracy: 0.5078 - val_loss: 1.6654 - val_accuracy: 0.5020\n",
      "Epoch 23/30\n",
      "3554/3554 [==============================] - 38s 11ms/step - loss: 1.6539 - accuracy: 0.5074 - val_loss: 1.6649 - val_accuracy: 0.5022\n",
      "Epoch 24/30\n",
      "3554/3554 [==============================] - 40s 11ms/step - loss: 1.6520 - accuracy: 0.5076 - val_loss: 1.6640 - val_accuracy: 0.5024\n",
      "Epoch 25/30\n",
      "3554/3554 [==============================] - 40s 11ms/step - loss: 1.6502 - accuracy: 0.5078 - val_loss: 1.6645 - val_accuracy: 0.5047\n",
      "Epoch 26/30\n",
      "3554/3554 [==============================] - 41s 12ms/step - loss: 1.6488 - accuracy: 0.5078 - val_loss: 1.6634 - val_accuracy: 0.5039\n",
      "Epoch 27/30\n",
      "3554/3554 [==============================] - 40s 11ms/step - loss: 1.6467 - accuracy: 0.5092 - val_loss: 1.6622 - val_accuracy: 0.5010\n",
      "Epoch 28/30\n",
      "3554/3554 [==============================] - 41s 11ms/step - loss: 1.6454 - accuracy: 0.5095 - val_loss: 1.6593 - val_accuracy: 0.5022\n",
      "Epoch 29/30\n",
      "3554/3554 [==============================] - 41s 12ms/step - loss: 1.6441 - accuracy: 0.5083 - val_loss: 1.6588 - val_accuracy: 0.5039\n",
      "Epoch 30/30\n",
      "3554/3554 [==============================] - 40s 11ms/step - loss: 1.6431 - accuracy: 0.5092 - val_loss: 1.6557 - val_accuracy: 0.5091\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model_RNN2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_RNN2.fit(X_train, y_train, epochs=30, verbose=1, validation_data=(X_test, y_test))\n",
    "model_RNN2.save_weights(\"char_rnn_model_weights_v1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3554/3554 [==============================] - 37s 10ms/step - loss: 1.6380 - accuracy: 0.5120\n",
      "Epoch 2/2\n",
      "3554/3554 [==============================] - 38s 11ms/step - loss: 1.6361 - accuracy: 0.5125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e26ec34a00>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightsfile_model_RNN2= Data_path+\"\\\\Pre_trained_models\\\\char_rnn_model_weights_v1.hdf5\"\n",
    "model_RNN2.load_weights(weightsfile_model_RNN2)\n",
    "\n",
    "# compile network\n",
    "model_RNN2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_RNN2.fit(X_train, y_train, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to prepare test input\n",
    "def prepare_input(in_text):\n",
    "    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n",
    "    X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "    return(X1)\n",
    "#function to loop our preditions\n",
    "def complete_pred(in_text):\n",
    "    #original_text = in_text\n",
    "    #generated = in_text\n",
    "    completion = ''\n",
    "    while True:\n",
    "        x = prepare_input(in_text)\n",
    "        pred = model_RNN2.predict_classes(x, verbose=0)[0]\n",
    "\n",
    "        next_char = indices_char[pred]\n",
    "\n",
    "        in_text = in_text[1:] + next_char\n",
    "        completion += next_char\n",
    "\n",
    "        if len(completion)> 20 or next_char == ' ':\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREEHARI\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text --> officials say  \n",
      "predicted word --->  to \n",
      "Input text --> how dangerous  \n",
      "predicted output --->  of \n",
      "Input text --> political and  \n",
      "predicted output --->  the \n",
      "Input text --> whatever they  \n",
      "predicted output --->  the \n",
      "Input text --> of particular  \n",
      "predicted output --->  to \n"
     ]
    }
   ],
   "source": [
    "in_text = 'officials say '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted word ---> \", out_word)\n",
    "in_text = 'how dangerous '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'political and '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'whatever they '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'of particular '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               84992     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 37)                4773      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 37)                0         \n",
      "=================================================================\n",
      "Total params: 89,765\n",
      "Trainable params: 89,765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building the model\n",
    "model_LSTM = Sequential()\n",
    "#model1.add(LSTM('number of hidden nodes in each rnn cell', input_shape=(timesteps, data_dim)))\n",
    "model_LSTM.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]))) \n",
    "model_LSTM.add(Dense(len(char_indices)))\n",
    "model_LSTM.add(Activation('softmax'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3554/3554 [==============================] - 17s 4ms/step - loss: 1.9520 - accuracy: 0.4304\n",
      "Epoch 2/30\n",
      "3554/3554 [==============================] - 16s 4ms/step - loss: 1.5527 - accuracy: 0.5372\n",
      "Epoch 3/30\n",
      "3554/3554 [==============================] - 16s 5ms/step - loss: 1.3849 - accuracy: 0.5876\n",
      "Epoch 4/30\n",
      "3554/3554 [==============================] - 17s 5ms/step - loss: 1.2700 - accuracy: 0.6200\n",
      "Epoch 5/30\n",
      "3554/3554 [==============================] - 17s 5ms/step - loss: 1.1837 - accuracy: 0.6454\n",
      "Epoch 6/30\n",
      "3554/3554 [==============================] - 16s 4ms/step - loss: 1.1157 - accuracy: 0.6660\n",
      "Epoch 7/30\n",
      "3554/3554 [==============================] - 15s 4ms/step - loss: 1.0600 - accuracy: 0.6827\n",
      "Epoch 8/30\n",
      "3554/3554 [==============================] - 16s 5ms/step - loss: 1.0126 - accuracy: 0.6963\n",
      "Epoch 9/30\n",
      "3554/3554 [==============================] - 17s 5ms/step - loss: 0.9708 - accuracy: 0.7077\n",
      "Epoch 10/30\n",
      "3554/3554 [==============================] - 15s 4ms/step - loss: 0.9340 - accuracy: 0.7184\n",
      "Epoch 11/30\n",
      "3554/3554 [==============================] - 15s 4ms/step - loss: 0.9025 - accuracy: 0.7270\n",
      "Epoch 12/30\n",
      "3554/3554 [==============================] - 17s 5ms/step - loss: 0.8718 - accuracy: 0.7360\n",
      "Epoch 13/30\n",
      "3554/3554 [==============================] - 17s 5ms/step - loss: 0.8458 - accuracy: 0.7428\n",
      "Epoch 14/30\n",
      "3554/3554 [==============================] - 16s 4ms/step - loss: 0.8211 - accuracy: 0.7485\n",
      "Epoch 15/30\n",
      "3554/3554 [==============================] - 16s 4ms/step - loss: 0.7988 - accuracy: 0.7554\n",
      "Epoch 16/30\n",
      "3554/3554 [==============================] - 15s 4ms/step - loss: 0.7780 - accuracy: 0.7615\n",
      "Epoch 17/30\n",
      "3554/3554 [==============================] - 16s 4ms/step - loss: 0.7594 - accuracy: 0.7653\n",
      "Epoch 18/30\n",
      "3554/3554 [==============================] - 16s 5ms/step - loss: 0.7430 - accuracy: 0.7702\n",
      "Epoch 19/30\n",
      "3554/3554 [==============================] - 16s 5ms/step - loss: 0.7248 - accuracy: 0.7746\n",
      "Epoch 20/30\n",
      "3554/3554 [==============================] - 17s 5ms/step - loss: 0.7111 - accuracy: 0.7780\n",
      "Epoch 21/30\n",
      "3554/3554 [==============================] - 16s 4ms/step - loss: 0.6995 - accuracy: 0.7816\n",
      "Epoch 22/30\n",
      "3554/3554 [==============================] - 15s 4ms/step - loss: 0.6864 - accuracy: 0.7844\n",
      "Epoch 23/30\n",
      "3554/3554 [==============================] - 16s 5ms/step - loss: 0.6763 - accuracy: 0.7887\n",
      "Epoch 24/30\n",
      "3554/3554 [==============================] - 16s 4ms/step - loss: 0.6641 - accuracy: 0.7914\n",
      "Epoch 25/30\n",
      "3554/3554 [==============================] - 15s 4ms/step - loss: 0.6555 - accuracy: 0.7931\n",
      "Epoch 26/30\n",
      "3554/3554 [==============================] - 14s 4ms/step - loss: 0.6473 - accuracy: 0.7964\n",
      "Epoch 27/30\n",
      "3554/3554 [==============================] - 16s 5ms/step - loss: 0.6379 - accuracy: 0.7980\n",
      "Epoch 28/30\n",
      "3554/3554 [==============================] - 17s 5ms/step - loss: 0.6282 - accuracy: 0.8013\n",
      "Epoch 29/30\n",
      "3554/3554 [==============================] - 16s 5ms/step - loss: 0.6235 - accuracy: 0.8013\n",
      "Epoch 30/30\n",
      "3554/3554 [==============================] - 15s 4ms/step - loss: 0.6156 - accuracy: 0.8041\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_LSTM.fit(X_train, y_train, epochs=30, verbose=1)\n",
    "model_LSTM.save_weights(\"char_LSTM_model_weights_v1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3554/3554 [==============================] - 17s 4ms/step - loss: 0.7146 - accuracy: 0.7902\n",
      "Epoch 2/2\n",
      "3554/3554 [==============================] - 17s 5ms/step - loss: 0.6708 - accuracy: 0.7954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e16981e8b0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightsfile_model_LSTM= Data_path+\"\\\\Pre_trained_models\\\\char_LSTM_model_weights_v1.hdf5\"\n",
    "model_LSTM.load_weights( weightsfile_model_LSTM)\n",
    "\n",
    "# compile network\n",
    "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_LSTM.fit(X_train, y_train,epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to prepare test input\n",
    "def prepare_input1(in_text):\n",
    "    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n",
    "    X1= keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "    return(X1)\n",
    "#function to loop our preditions\n",
    "def complete_pred1(in_text):\n",
    "    #original_text = in_text\n",
    "    #generated = in_text\n",
    "    completion = ''\n",
    "    while True:\n",
    "        x = prepare_input1(in_text)\n",
    "        pred = model_LSTM.predict_classes(x, verbose=0)[0]\n",
    "        next_char = indices_char[pred]\n",
    "\n",
    "        in_text = in_text[1:] + next_char\n",
    "        completion += next_char\n",
    "\n",
    "        if len(completion)> 20 or next_char == ' ':\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text --> the emergence  ; predicted output --->  of \n",
      "Input text --> officials say  ; predicted output --->  that \n",
      "Input text --> and sentenced  ; predicted output --->  to \n",
      "Input text --> a combination  ; predicted output --->  of \n",
      "Input text --> and according  ; predicted output --->  to \n"
     ]
    }
   ],
   "source": [
    "in_text = 'the emergence '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'officials say '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'and sentenced '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'a combination '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'and according '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few more predictions and Comparions with Standard RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text --> how dangerous  \n",
      "LSTM Prediction --->  is \n",
      "RNN Prediction --->  of \n",
      "\n",
      "\n",
      "Input text --> political and  \n",
      "LSTM Prediction --->  economic \n",
      "RNN Prediction --->  the \n",
      "\n",
      "\n",
      "Input text --> of particular  \n",
      "LSTM Prediction --->  importance \n",
      "RNN Prediction --->  to \n",
      "\n",
      "\n",
      "Input text --> whatever they  \n",
      "LSTM Prediction --->  want \n",
      "RNN Prediction --->  the \n"
     ]
    }
   ],
   "source": [
    "in_text = 'how dangerous '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'political and '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'of particular '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'whatever they '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study  Language Translation Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Go.' 'Va !']\n",
      " ['Run!' 'Cours\\u202f!']\n",
      " ['Run!' 'Courez\\u202f!']\n",
      " ...\n",
      " ['Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.'\n",
      "  \"Puisqu'il y a de multiples sites web sur chaque sujet, je clique d'habitude sur le bouton retour arrire lorsque j'atterris sur n'importe quelle page qui contient des publicits surgissantes. Je me rends juste sur la prochaine page propose par Google et espre tomber sur quelque chose de moins irritant.\"]\n",
      " [\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\"\n",
      "  \"Si quelqu'un qui ne connat pas vos antcdents dit que vous parlez comme un locuteur natif, cela veut dire qu'il a probablement remarqu quelque chose  propos de votre locution qui l'a fait prendre conscience que vous n'tes pas un locuteur natif. En d'autres termes, vous ne parlez pas vraiment comme un locuteur natif.\"]\n",
      " ['It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.'\n",
      "  \"Il est peut-tre impossible d'obtenir un Corpus compltement dnu de fautes, tant donne la nature de ce type d'entreprise collaborative. Cependant, si nous encourageons les membres  produire des phrases dans leurs propres langues plutt que d'exprimenter dans les langues qu'ils apprennent, nous pourrions tre en mesure de rduire les erreurs.\"]]\n",
      "Overall pairs 135842\n"
     ]
    }
   ],
   "source": [
    "raw_data= open(Data_path+\"\\\\Datasets\\\\fra-eng\\\\eng-fra.txt\", mode='rt', encoding='utf-8').read()\n",
    "raw_data=raw_data.strip().split('\\n')\n",
    "raw_data=[i.split('\\t') for i in raw_data]\n",
    "lang1_lang2_data=array(raw_data)\n",
    "print(lang1_lang2_data)\n",
    "print(\"Overall pairs\", len(lang1_lang2_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Go' 'Va ']\n",
      " ['Run' 'Cours\\u202f']\n",
      " ['Run' 'Courez\\u202f']\n",
      " ...\n",
      " ['Since there are usually multiple websites on any given topic I usually just click the back button when I arrive on any webpage that has popup advertising I just go to the next page found by Google and hope for something less irritating'\n",
      "  'Puisquil y a de multiples sites web sur chaque sujet je clique dhabitude sur le bouton retour arrire lorsque jatterris sur nimporte quelle page qui contient des publicits surgissantes Je me rends juste sur la prochaine page propose par Google et espre tomber sur quelque chose de moins irritant']\n",
      " ['If someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker In other words you dont really sound like a native speaker'\n",
      "  'Si quelquun qui ne connat pas vos antcdents dit que vous parlez comme un locuteur natif cela veut dire quil a probablement remarqu quelque chose  propos de votre locution qui la fait prendre conscience que vous ntes pas un locuteur natif En dautres termes vous ne parlez pas vraiment comme un locuteur natif']\n",
      " ['It may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort However if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors'\n",
      "  'Il est peuttre impossible dobtenir un Corpus compltement dnu de fautes tant donne la nature de ce type dentreprise collaborative Cependant si nous encourageons les membres  produire des phrases dans leurs propres langues plutt que dexprimenter dans les langues quils apprennent nous pourrions tre en mesure de rduire les erreurs']]\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "lang1_lang2_data[:,0] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,0]]\n",
    "lang1_lang2_data[:,1] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,1]]\n",
    "\n",
    "print(lang1_lang2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go' 'va ']\n",
      " ['run' 'cours\\u202f']\n",
      " ['run' 'courez\\u202f']\n",
      " ...\n",
      " ['since there are usually multiple websites on any given topic i usually just click the back button when i arrive on any webpage that has popup advertising i just go to the next page found by google and hope for something less irritating'\n",
      "  'puisquil y a de multiples sites web sur chaque sujet je clique dhabitude sur le bouton retour arrire lorsque jatterris sur nimporte quelle page qui contient des publicits surgissantes je me rends juste sur la prochaine page propose par google et espre tomber sur quelque chose de moins irritant']\n",
      " ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker'\n",
      "  'si quelquun qui ne connat pas vos antcdents dit que vous parlez comme un locuteur natif cela veut dire quil a probablement remarqu quelque chose  propos de votre locution qui la fait prendre conscience que vous ntes pas un locuteur natif en dautres termes vous ne parlez pas vraiment comme un locuteur natif']\n",
      " ['it may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort however if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors'\n",
      "  'il est peuttre impossible dobtenir un corpus compltement dnu de fautes tant donne la nature de ce type dentreprise collaborative cependant si nous encourageons les membres  produire des phrases dans leurs propres langues plutt que dexprimenter dans les langues quils apprennent nous pourrions tre en mesure de rduire les erreurs']]\n"
     ]
    }
   ],
   "source": [
    "## convert text to lowercase\n",
    "for word in range(len(lang1_lang2_data)):\n",
    "    lang1_lang2_data[word,0] = lang1_lang2_data[word,0].lower()\n",
    "    lang1_lang2_data[word,1] = lang1_lang2_data[word,1].lower()\n",
    "print(lang1_lang2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang1_vocab_size 13711\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lang1_lang2_data[:, 0])\n",
    "lang1_tokens=tokenizer\n",
    "lang1_vocab_size = len(lang1_tokens.word_index) + 1\n",
    "print(\"lang1_vocab_size\", lang1_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang2_vocab_size 29664\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lang1_lang2_data[:, 1])\n",
    "lang2_tokens=tokenizer\n",
    "lang2_vocab_size = len(lang2_tokens.word_index) + 1\n",
    "print(\"lang2_vocab_size\", lang2_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set\n",
    "train, test = train_test_split(lang1_lang2_data, test_size=0.1, random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (122257, 15)\n",
      "Y_train.shape (122257, 15)\n",
      "X_test.shape (13585, 15)\n",
      "Y_test.shape (13585, 15)\n"
     ]
    }
   ],
   "source": [
    "lang1_seq_length=15\n",
    "lang2_seq_length=15\n",
    "\n",
    "X_train_seq=lang1_tokens.texts_to_sequences(train[:, 0])\n",
    "X_train= pad_sequences(X_train_seq,lang1_seq_length,padding='post')\n",
    "\n",
    "Y_train_seq=lang2_tokens.texts_to_sequences(train[:, 1])\n",
    "Y_train= pad_sequences(Y_train_seq,lang2_seq_length,padding='post')\n",
    "\n",
    "X_test_seq=lang1_tokens.texts_to_sequences(test[:, 0])\n",
    "X_test= pad_sequences(X_test_seq,lang1_seq_length,padding='post')\n",
    "\n",
    "Y_test_seq=lang2_tokens.texts_to_sequences(test[:, 1])\n",
    "Y_test= pad_sequences(Y_test_seq,lang2_seq_length,padding='post')\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"Y_train.shape\",Y_train.shape)\n",
    "print(\"X_test.shape\",X_test.shape)\n",
    "print(\"Y_test.shape\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data ['this song is familiar to us']\n",
      "Numbers sequence [17, 615, 6, 1156, 3, 87]\n",
      "Padded Sequence [  17  615    6 1156    3   87    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Text data\", [train[5, 0]])\n",
    "print('Numbers sequence', X_train_seq[5])\n",
    "print('Padded Sequence', X_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 15, 256)           3510016   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 15, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 15, 29664)         3826656   \n",
      "=================================================================\n",
      "Total params: 7,665,376\n",
      "Trainable params: 7,665,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(lang1_vocab_size, 256, input_length=lang1_seq_length, mask_zero=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(RepeatVector(lang2_seq_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dense(lang2_vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 21s 141ms/step - loss: 5.0375\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "history = model.fit(X_train, Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1),  epochs=1, verbose=1, batch_size=1024)\n",
    "model.save_weights('Eng_fra_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"C:\\\\Users\\\\SREEHARI\\\\Desktop\\\\internship\\\\my training\\\\Chapter12_RNN_LSTM_V4\\\\Eng_fra_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_line_prediction(text1):\n",
    "    \n",
    "    def to_lines(text):\n",
    "          sents = text.strip().split('\\n')\n",
    "          sents = [i.split('\\t') for i in sents]\n",
    "          return sents\n",
    "    small_input = to_lines(text1)\n",
    "    small_input = array(small_input)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    small_input[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in small_input[:,0]]\n",
    "    # convert text to lowercase\n",
    "    for i in range(len(small_input)):\n",
    "        small_input[i,0] = small_input[i,0].lower()\n",
    "\n",
    "    #encode and pad sequences\n",
    "    small_input_seq=lang1_tokens.texts_to_sequences(small_input[0])\n",
    "    small_input= pad_sequences(small_input_seq,lang1_seq_length,padding='post')\n",
    "   \n",
    "\n",
    "    #Load the model\n",
    "    #Eng French Model\n",
    "    #model.load_weights('C:\\\\Users\\\\SREEHARI\\\\Desktop\\\\internship\\\\my training\\\\Chapter12_RNN_LSTM_V4\\\\Eng_fra_model.hdf5')\n",
    "\n",
    "    pred_seq = model.predict_classes(small_input[0:1].reshape((small_input[0:1].shape[0],small_input[0:1].shape[1])))\n",
    "    \n",
    "    def num_to_word(n, tokens):\n",
    "          for word, index in tokens.word_index.items():\n",
    "              if index == n:\n",
    "                  return word\n",
    "          return None\n",
    "\n",
    "    Lang2_text = []\n",
    "    for word_num in pred_seq:\n",
    "          sing_pred = []\n",
    "          for i in range(len(word_num)):\n",
    "                t = num_to_word(word_num[i], lang2_tokens)\n",
    "                if i > 0:\n",
    "                    if (t == num_to_word(word_num[i-1], lang2_tokens)) or (t == None):\n",
    "                        sing_pred.append('')\n",
    "                    else:\n",
    "                        sing_pred.append(t)\n",
    "                else:\n",
    "                      if(t == None):\n",
    "                              sing_pred.append('')\n",
    "                      else:\n",
    "                              sing_pred.append(t) \n",
    "          Lang2_text.append(' '.join(sing_pred))\n",
    "    return(Lang2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have a great Good day']  --> ['              ']\n",
      "['Do you speak English']  --> ['              ']\n",
      "['I do not know your language']  --> ['              ']\n",
      "['I need help']  --> ['              ']\n",
      "['Thank you very much']  --> ['              ']\n",
      "['Where can I get this']  --> ['              ']\n",
      "['How much does it cost']  --> ['              ']\n",
      "['Where is the bathroom']  --> ['              ']\n",
      "['Where is the ATM']  --> ['              ']\n",
      "['I am a visitor here']  --> ['              ']\n",
      "['Excuse me']  --> ['              ']\n",
      "['What do you do for living']  --> ['              ']\n",
      "['Here is my passport']  --> ['              ']\n"
     ]
    }
   ],
   "source": [
    "Input_sentences=[\"have a great Good day\",\n",
    "                 \"Do you speak English\",\n",
    "                 \"I do not know your language\",\n",
    "                 \"I need help\",\n",
    "                 \"Thank you very much\",\n",
    "                 \"Where can I get this\",\n",
    "                 \"How much does it cost\",\n",
    "                 \"Where is the bathroom\",\n",
    "                 \"Where is the ATM\",\n",
    "                 \"I am a visitor here\",\n",
    "                 \"Excuse me\",\n",
    "                 \"What do you do for living\",\n",
    "                 \"Here is my passport\"]\n",
    "\n",
    "for sent in Input_sentences:\n",
    "  print([sent] , \" -->\",one_line_prediction(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
